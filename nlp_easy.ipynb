{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/electromax/Data/jupyter/diploma/github_gold.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4063186</td>\n",
       "      <td>neutral</td>\n",
       "      <td>No. I still see the wrong twins.  * https://gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3894703</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Reverted.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971084</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You can leave a queue while in queue ? (before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1827828</td>\n",
       "      <td>positive</td>\n",
       "      <td>Didn't look at SpellTargetRestrictions XD\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232603</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not sure about what kind of line lengths the p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Polarity                                               Text\n",
       "0  4063186   neutral  No. I still see the wrong twins.  * https://gi...\n",
       "1  3894703   neutral                                         Reverted.\"\n",
       "2  1971084   neutral  You can leave a queue while in queue ? (before...\n",
       "3  1827828  positive         Didn't look at SpellTargetRestrictions XD\"\n",
       "4   232603   neutral  Not sure about what kind of line lengths the p..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "clean_train_reviews = [review_to_words(review) for review in train['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   # already preprocessed\n",
    "                             max_features = 10000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    train_data_features,\n",
    "    train[\"sentiment\"], \n",
    "    test_size=0.2,\n",
    "    random_state=241\n",
    ")\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "def get_classifier_score(classifier):\n",
    "    predictions = classifier.predict(X_test)\n",
    "    score = sklearn.metrics.accuracy_score(y_test, predictions)\n",
    "    return score\n",
    "\n",
    "split_generator = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "def cross_validation_mean_score(clf):\n",
    "\n",
    "    score_arr = sklearn.model_selection.cross_val_score(\n",
    "        estimator=clf,\n",
    "        X=train_data_features,\n",
    "        y=train[\"sentiment\"],\n",
    "        cv=split_generator,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    score = score_arr.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "forest = forest.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8422\n"
     ]
    }
   ],
   "source": [
    "print(get_classifier_score(forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the test data\n",
    "test = pd.read_csv(\"./input/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "\n",
    "clean_test_reviews = [review_to_words(review) for review in test['review']]\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"./Bag_of_Words_model_submission.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96844\n"
     ]
    }
   ],
   "source": [
    "print(forest.score(train_data_features, train[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "positive_review = vectorizer.transform([review_to_words(\"this film was amazing! i couldnt stop watching\")])\n",
    "negative_review = vectorizer.transform([review_to_words(\"this film was horrible! i stopped watching immediately\")])\n",
    "\n",
    "print(forest.predict(negative_review))\n",
    "print(forest.predict(positive_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "score: 0.84564"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30219463ed35f1d7a1669aa5bfb2f24770e563bdf7eb6c65f89c27192c9e55b7"
  },
  "kernelspec": {
   "display_name": "jptr",
   "language": "python",
   "name": "jptr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
